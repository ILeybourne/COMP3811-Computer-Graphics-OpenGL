\chapter{Relevant Materials}
\label{ch:2relevantmaterials}

\section{Relevant Literature}
\label{sec:relevantliterature}
\subsection{Generative Adversarial Networks (GANs)}
\label{subsec:generativeadversarialnetworks}
Generative Adversarial Networks were formally introduced in 2014. Unlike most other forms of machine learning, that are normally posed as optimisation problems, GANs are heavily based on game theory. A GAN is composed of two models which can be thought of as 'players', a generator and a discriminator. The generator aims to generate new data point based on the given input dataset, while the discriminator seeks to accurately identify the authenticity of the generated data points. This gives rise to an adversarial game between the two networks whereby the generator tries to generate new outputs that will deceive the discriminator. 

\noindent The two networks are trained together, with each of the two players experiencing a cost indicated by a cost function relating to its performance. The generator tries to minimise cost by creating data points that are similar enough to the dataset that they are classified as genuine by the discriminator. All the while, the discriminator is trying to minimise its cost by correctly classifying the newly created data points. \cite{goodfellow2020generative}

\noindent This method enables newly generated data points to be increasingly more authentic, while still being novel and distinct from the original dataset. 

\noindent GANs are capable of imitating complex datasets such as dataset containing image files or audio waveforms. GANs have been demonstrated to be able to create realistic photographs \cite{brock2019large} and new artworks
\cite{brock2019large} \cite{karras2019stylebased} \cite{artbreeder} 

\subsection{Conditional Generative Adversarial Networks (cGANs)}
\label{subsec:conditionalgenerativeadversarialnetworks}
Later in 2014, Conditional Generative Adversarial Networks were formally introduced. They work in a similar way to conventional, unconditional GANs but also allow for extra conditional data to be introduced to the model. This additional data is used to condition the GAN to output specific traits based on the conditions. The conditions are then inputted to both the generator and discriminator \cite{mirza2014conditional}.

\noindent Pix2Pix \cite{isola2018imagetoimage} is a popular framework that uses CGAN technology to solve general-purpose image-to-image translation problems. In their paper, Isola et al. (2018) acknowledge how "a large number of internet users (many of them artists)
have posted their own experiments with our system, further
demonstrating its wide applicability and ease of adoption
without the need for parameter tweaking" and demonstrate how GANs can make the creation of digital art more accessible. An interactive demo using four pre-trained models is available online \cite{imagetoimagedemo}. 

\subsection{Image to Image Translation}
\label{subsec:imagetoimagetranslation}
Image to image translation is a type of graphics problem that involves mapping an input image to an output image. CycleGAN \cite{zhu2020unpairedcyclegan} is a model that specialises in unpaired image translation. This means that we do not need a paired database, so this model can perform well even when a matched database does not exist. This is demonstrated by CycleGAN in an example whereby they transferred images of horses onto zebras and zebras onto horses from two separate datasets. Liu et al. (2018) \cite{liu2018unsupervisedimagetoimage} showed in their paper, titled "Unsupervised Image-to-Image Translation Networks", that scenes could be transformed from winter to summer scenes and vice versa. 

\noindent Another use of image to image translation models is applying style transfers. CycleGAN showed that photographs could be rendered using the styles of various artists such as Monet, Van Gogh, Cezanne and Ukiyo-e \cite{zhu2020unpairedcyclegan}.

\subsection{Periodic Spatial GAN (PSGAN)}
\label{subsec:periodicspatialgan}
In a paper called, "Learning Texture Manifolds with the Periodic Spatial GAN", Bergmann et al. (2017) \cite{bergmann2017learningtexturemanifold} showed how their PSGAN performed highly on periodic repeating textures found within the Oxford Describable Textures Dataset. The PSGAN was capable of learning the overall patterns of the texture images and could reproduce realistic outputs with much more authenticity than similar models.

\noindent As we want similar results that generate material textures that we can use to texture our UV maps. This paper may come in useful in helping to solve objective 2 declared in \nameref{subsec:objectives}.

\subsection{Semantic Layouts}
\label{subsec:semanticlayouts}
In a groundbreaking paper, Park et al. (2019), revealed how photo-realistic images could be generated from basic semantic layouts \cite{park2019semantic}. The semantic layout was used as an input for the model. A label map was used to identify what type of object should be rendered, and its pixel position in the input image was used to acknowledge where the object should be rendered in the output image. This paper was a massive inspiration to this project and motivated us to use a semantic layout method of integrating multiple textures onto a UV map.

\subsection{UV Mapping}
\label{subsec:uvmapping}
A UV map is a representation of a 3D model's faces in a flat, 2D form. Texture mapping is performed so that the faces can be textured with 2D images. The relative parts of the map then correspond with the parts of the images. There are many different ways of generating UV maps, with some maps being preferable over others \cite{interactivetexturemapping}. 

This is one of the more challenging aspects of the project and which is why we have decided to manually generate the UV beforehand and then implement them into the application. UV maps could automatically be generated but unfortunately with the time and resources allocated this is unfeasible.

\section{APIs}
\label{sec:apis}
\subsection{Pix2Pix}
\label{subsec:pix2pix}
The Pix2Pix model is a cGAN capable of performing a wide range of image to image translation tasks. It has demonstrated that it can generate images of street scenes or buildings from semantic layouts, accurately colour greyscale images, generate maps from satellite images, applying style transfers, and generate realistic images of objects from input sketches \cite{isola2018imagetoimage}.

\noindent It is a powerful and relevant framework to use for a project such as this one.

\subsection{PyTorch}
\label{subsec:pytorch}
PyTorch is a popular open-source machine learning framework that can be used for computer vision tasks. Pix2Pix has a version implemented with Pytorch so we have opted to use it for the sake on consistency \cite{pytorch}. 

\subsection{NumPy}
\label{subsec:numpy}
NumPy is a Python-based library used for performing numerical computation with multi-dimensional arrays. It is a useful library to use with machine learning projects and will simplify the project \cite{numpy}.

\subsection{Qt}
\label{subsec:qt}
Qt is a Graphical User Interface (GUI) framework that allows for the creation of interactive GUIs. The application will require a GUI for the end-user to interact with. Qt was chosen for its simplicity and flexibility. Documentation and more information can be found on \href{https://www.qt.io/}{their website}. 

\subsection{OpenGL}
\label{subsec:opengl}
OpenGL (Open Graphics Library) cross-platform API for developing 2D and 3D graphical applications \cite{openglabout}. It is an environment we have experience using and was chosen since it is free, easy to use, well documented and is supported by Qt - via the Qt OpenGL module \cite{qtopenglmodule}.

\section{Resources}
\label{sec:resources}
\subsection{Textures}
\label{subsec:textures}
For the texture datasets, I will be using Oxford's "Describable Textures Dataset" (DTD) \cite{cimpoi14describingtextures} since it is a well-labelled dataset "made available to the computer vision community for research purposes" \cite{describingtexturesweb}. It is a diverse dataset of 5640 images, separated into 47 categories. In addition I will be using the "Flickr Material Database" \cite{materialperceptiondataset} a material dataset of 1000 images with 10 different categories.
The size of these datasets are manageable but still large enough to obtain diverse output images via the GAN model.

\subsection{3D Models}
\label{subsec:3dmodels}
Models used from this project will be sourced from Princeton's ModelNet \cite{volumetricshapesmodeldataset}. The dataset contains 127,915 3D models stored in Object File Format (OFF). The models are split into 662 categories. It is a particularly large dataset so it will be pruned to reduce the amount of hard drive space need to store the models. Additional information about ModelNet can be found on \href{https://modelnet.cs.princeton.edu/}{their website}.